{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0bnVyLC8eNXYFQn7GVSqC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshkotkar/Deep-Learning/blob/main/advance_rag(webbase_search).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GLl6NKaW9oE",
        "outputId": "e0eb4f1a-3acb-4d30-ca8c-51b36b1030ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 1.0.2\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://docs.langchain.com/\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: langchain-core, langgraph, pydantic\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv\n",
        "!pip install wikipedia\n",
        "!pip install langchain-google-genai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHXEi5JqaBMg",
        "outputId": "6f0ce779-cccd-42c1-b7c7-b245a1c6c163"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.12/dist-packages (2.2.0)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.12/dist-packages (from arxiv) (6.0.12)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.12/dist-packages (from arxiv) (2.32.5)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2025.10.5)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (4.15.0)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (3.0.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.0.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.11.10)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (2.26.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.75.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (0.4.37)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (4.15.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (2.32.5)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community\n",
        "!pip install langchain-text-splitter\n",
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJZRSI8sl9ZX",
        "outputId": "95466a15-32b3-485a-c47f-8965b94b3eaa"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.12/dist-packages (0.4)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (1.0.1)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.11.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.37)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_community) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_community) (4.15.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-text-splitter (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-text-splitter\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijbEaU-KnujH",
        "outputId": "089f27e5-c1d9-42c7-efd7-f31b10f9c9d1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# üîπ Enable LangSmith tracing\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "\n",
        "# üîπ Ask user for their LangSmith API key (securely)\n",
        "langsmith_key = getpass.getpass(\"üîë Enter your LangSmith API key: \")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_key\n",
        "\n",
        "print(\"‚úÖ Environment variables set for LangSmith \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK8WTxdMySCn",
        "outputId": "dac95935-574e-443a-ea36-7bb5c0f7a19b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë Enter your LangSmith API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ Environment variables set for LangSmith \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import WikipediaQueryRun# langchain_community consiist all the tools are available in langchain\n",
        "from langchain_community.utilities import WikipediaAPIWrapper"
      ],
      "metadata": {
        "id": "yCyuu46NmEMP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_api=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=300)#basically it is interacting with wiki to find the multipule results and then it is returning the top k results based on our query\n",
        "wiki=WikipediaQueryRun(api_wrapper=wiki_api)# it is taking the wiki api wrapper as input and then it is returning the tool which we can use to query the wikipedia"
      ],
      "metadata": {
        "id": "4euMhmJ2mbBY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HtmhKg-xm0gJ",
        "outputId": "4f0d346d-590a-4bf3-e9b4-0d66e67bc279"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wikipedia'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS#use to create the vector storage\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter #split text into chunks\n",
        "embeddings=HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\",\n",
        "                                 model_kwargs={\"device\":\"cpu\"},\n",
        "                                 encode_kwargs={\"normalize_embeddings\":True})\n",
        "#create the text into numberical embeddings with semantic meaning in 768 dimension vector\n",
        "\n",
        "loder=WebBaseLoader(\"https://docs.langchain.com/langsmith/home\")#it is used to load the data from the web page\n",
        "docs=loder.load()#it is used to load the data from the web page\n",
        "\n",
        "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs) #it is used to split the text into chunks of 1000 characters with an overlap of 200 characters overlap means that last 200 characters of previous chunk will be same as first 200 characters of next chunk\n",
        "vectordb=FAISS.from_documents(documents,embedding=embeddings)#it is used to create the vector storage from the documents and embeddings\n",
        "retriver=vectordb.as_retriever()#it is a interface which is used to retrieve the documents from the vector storage based on the query\n",
        "retriver"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5HcP0cbm1SP",
        "outputId": "5de01ca5-fdfa-4437-9ab4-c54f0857f9f3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7e17d6535f70>, search_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "# loder=WebBaseLoader(\"https://docs.langchain.com/langsmith/home\")#it is used to load the data from the web page\n",
        "# docs=loder.load()#it is used to load the data from the web page\n",
        "\n",
        "# documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs) #it is used to split the text into chunks of 1000 characters with an overlap of 200 characters overlap means that last 200 characters of previous chunk will be same as first 200 characters of next chunk\n",
        "# embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "# vectordb=FAISS.from_documents(documents,embedding=embeddings)#it is used to create the vector storage from the documents and embeddings\n",
        "# retriver=vectordb.as_retriever()#it is a interface which is used to retrieve the documents from the vector storage based on the query\n",
        "# retriver\n"
      ],
      "metadata": {
        "id": "eBFb5ln2nIWy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create retriver tool use to do retieval of the documents\n",
        "from langchain_core.tools import create_retriever_tool\n",
        "retriever_tool=create_retriever_tool(retriever=retriver,name=\"langsmith_search\",\n",
        "                      description='''Search for information about the langsmith. for any question about langsmith, you must use this tool''')"
      ],
      "metadata": {
        "id": "05-tt4L1nJUn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_tool.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AqTAbvupnS_v",
        "outputId": "9703ef3b-cde0-4c24-80c0-50f71c52cf23"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'langsmith_search'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hk8oIBzkqc5v"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arxiv tool\n",
        "from langchain_community.utilities import ArxivAPIWrapper\n",
        "from langchain_community.tools import ArxivQueryRun\n",
        "\n",
        "arxiv_wrapper=ArxivAPIWrapper(top_k_results=1,doc_content_chars_max=200)#it is used to interact with arxiv api to get the research papers based on our query, doc_content_chars_max is used to limit the number of characters in the document\n",
        "arxiv_tool=ArxivQueryRun(api_wrapper=arxiv_wrapper)#it is used to create the tool which we can use to query the arxiv api\n",
        "arxiv_tool.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nERBlfxAnToh",
        "outputId": "62cd8ecf-74ae-40b9-e5f4-b7fbb8113305"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'arxiv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools=[wiki,arxiv_tool,retriever_tool]"
      ],
      "metadata": {
        "id": "b0XXQzUfnVZX"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNi5t4F-nXNk",
        "outputId": "0b3b9342-6c4e-4110-87b5-4fb5e5df2b26"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/usr/local/lib/python3.12/dist-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=300)),\n",
              " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200)),\n",
              " Tool(name='langsmith_search', description='Search for information about the langsmith. for any question about langsmith, you must use this tool', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x7e17575e6700>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7e17d6535f70>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x7e1753d6f6a0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7e17d6535f70>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### agents will be responsible to use these tools to answer the questions basically in agents language model is used to decied which tool to use and when to use (like it use the reasonong engine to determine which action to take in which order) where the chains are used to perfrom specific tasks chains are hardcoded to a specific sequence of actions.\n",
        "\n",
        "# basicaly here in tools its like it will seach first in wiki if it dont find anything then it will search in arxiv if it dont find anything then it will search in langsmith documents retriver tool.üòÅ"
      ],
      "metadata": {
        "id": "tgnysWMHnY5f"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "if google_api_key:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0)\n"
      ],
      "metadata": {
        "id": "2vTtl1AHnbRY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    PromptTemplate\n",
        ")\n",
        "\n",
        "# System message: defines the assistant's role\n",
        "system_message = SystemMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[],\n",
        "        template=\"You are a helpful assistant that answers questions about LangSmith.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Placeholder for chat history (optional)\n",
        "chat_history = MessagesPlaceholder(variable_name=\"chat_history\", optional=True)\n",
        "\n",
        "# Human message: takes user input\n",
        "human_message = HumanMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"input\"],\n",
        "        template=\"{input}\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Scratchpad for intermediate agent steps\n",
        "scratchpad = MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "\n",
        "# Combine into a full chat prompt\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    system_message,\n",
        "    chat_history,\n",
        "    human_message,\n",
        "    scratchpad\n",
        "])\n"
      ],
      "metadata": {
        "id": "sq_MGRF1n-DB"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
        "# agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
        "# agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
        "\n",
        "# # Run the agent\n",
        "# response = agent_executor.invoke({\"input\": \"How do I trace a LangChain run with LangSmith?\"})\n",
        "# print(response)\n",
        "\n",
        "\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# from langchain.tools import tool\n",
        "# from langchain.agents import create_agent\n",
        "# from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# memory=MemorySaver()\n",
        "# agent=create_agent(model=llm,tools=tools,checkpointer=memory)\n",
        "# response = agent.invoke({\n",
        "#     \"messages\": [(\"human\", \"How do I trace a LangChain run with LangSmith?\")]\n",
        "# })\n",
        "\n",
        "# final msg\n",
        "# print(response[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "XCvvED9boURq"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# from langchain.tools import tool\n",
        "# from langgraph.prebuilt import create_react_agent\n",
        "# from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# agent = create_react_agent(model=llm, tools=tools,prompt=prompt)\n",
        "\n",
        "# response = agent.invoke({\n",
        "#     \"messages\": [(\"human\", \"what is LangSmith?\")]\n",
        "# })\n",
        "\n",
        "# print(response[\"messages\"][-1].content)\n",
        "\n",
        "# # from langchain.agents import AgentExecutor\n",
        "# # agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
        "# # agent_executor\n",
        "# # agent_executor.invoke({\"input\":\"tell me about langsmith\"})\n"
      ],
      "metadata": {
        "id": "4HY8Ge1Gxs5Y"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# from langgraph.prebuilt import create_react_agent\n",
        "# from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# tools = [wiki, arxiv_tool, retriever_tool]\n",
        "\n",
        "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0)\n",
        "\n",
        "# memory = MemorySaver()\n",
        "\n",
        "# agent = create_react_agent(\n",
        "#     model=llm,\n",
        "#     tools=tools,\n",
        "#     prompt=prompt,\n",
        "#     checkpointer=memory  # memory = conversation state\n",
        "# )\n",
        "\n",
        "# config = {\"configurable\": {\"thread_id\": \"harsh_langsmith_session_1\"}}\n",
        "\n",
        "# response = agent.invoke(\n",
        "#     {\"input\": \"What is LangSmith?\", \"agent_scratchpad\": \"\"},\n",
        "#     config=config\n",
        "# )\n",
        "\n",
        "# print( response[\"messages\"][-1].content)\n",
        "\n",
        "# response2 = agent.invoke(\n",
        "#     {\"input\": \"Who created it?\", \"agent_scratchpad\": \"\"},\n",
        "#     config=config\n",
        "# )\n",
        "\n",
        "# print( response2[\"messages\"][-1].content)\n"
      ],
      "metadata": {
        "id": "Bh5tma0J2VOO"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Memory\n",
        "memory = MemorySaver()\n",
        "\n",
        "agent = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=tools,\n",
        "    checkpointer=memory\n",
        ")\n",
        "\n",
        "#\n",
        "config = {\"configurable\": {\"thread_id\": \"session_1\"}}\n",
        "\n",
        "\n",
        "\n",
        "#final response\n",
        "response = agent.invoke(\n",
        "    {\"messages\": [(\"human\", \"What is paper 1605.08386 about?\")]},\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(\"\\n\\n Final answer:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l_Z5A7Y3D5x",
        "outputId": "23252d5f-c50d-47d9-d6a2-3fa0f2477e53"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1048840565.py:17: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  agent = create_react_agent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Final answer: {'messages': [HumanMessage(content='What is paper 1605.08386?', additional_kwargs={}, response_metadata={}, id='0f15f1bc-dd5b-4042-861c-fae8b19a7b7e'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'arxiv', 'arguments': '{\"query\": \"1605.08386\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-exp', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--ee3142ef-6f5e-454c-bf4f-0458964e796a-0', tool_calls=[{'name': 'arxiv', 'args': {'query': '1605.08386'}, 'id': '8867d702-993a-4372-b373-3cc9503ddbc8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 157, 'output_tokens': 12, 'total_tokens': 169, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='Published: 2016-05-26\\nTitle: Heat-bath random walks with Markov bases\\nAuthors: Caprice Stanley, Tobias Windisch\\nSummary: Graphs on lattice points are studied whose edges come from a finite set of\\nallo', name='arxiv', id='c00f5b51-3818-47a8-9d9c-1f2a31a3f6e3', tool_call_id='8867d702-993a-4372-b373-3cc9503ddbc8'), AIMessage(content='The paper 1605.08386, titled \"Heat-bath random walks with Markov bases,\" was published on 2016-05-26. The authors are Caprice Stanley and Tobias Windisch. The paper studies graphs on lattice points whose edges come from a finite set of allo.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-exp', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--d08e57ae-59ec-4160-bb7a-2cf754ba379c-0', usage_metadata={'input_tokens': 224, 'output_tokens': 68, 'total_tokens': 292, 'input_token_details': {'cache_read': 0}})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Tools\n",
        "tools = [wiki, arxiv_tool, retriever_tool]\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0)\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "#create Agent\n",
        "agent = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=tools,\n",
        "    checkpointer=memory\n",
        ")\n",
        "\n",
        "# config (for conversation thread)\n",
        "config = {\"configurable\": {\"thread_id\": \"harsh_langsmith_session_1\"}}\n",
        "\n",
        "# run the query\n",
        "response = agent.invoke(\n",
        "    {\"messages\": [(\"human\", \"What is paper 1605.08386 about?\")]},\n",
        "    config=config\n",
        ")\n",
        "\n",
        "# output format\n",
        "\n",
        "print(\"AGENT EXECUTION TRACE\")\n",
        "\n",
        "\n",
        "for msg in response[\"messages\"]:\n",
        "    if \"function_call\" in msg.additional_kwargs:\n",
        "        fn = msg.additional_kwargs[\"function_call\"][\"name\"]\n",
        "        args = msg.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "        print(f\"\\n Function Call ‚Üí {fn}({args})\")\n",
        "\n",
        "    # check if it's a tool's output\n",
        "    elif msg.__class__.__name__ == \"ToolMessage\":\n",
        "        print(\"\\n Tool Output:\")\n",
        "        print(msg.content.strip())\n",
        "\n",
        "# final ans\n",
        "final_answer = response[\"messages\"][-1].content\n",
        "print(\"\\n Final Answer:\")\n",
        "print(final_answer)\n",
        "print(\"=\" * 80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJSGw79W5Fhq",
        "outputId": "92a1793f-c39d-40b8-f4a6-3b0883bde8e3"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1597741302.py:17: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  agent = create_react_agent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AGENT EXECUTION TRACE\n",
            "\n",
            " Function Call ‚Üí arxiv({\"query\": \"1605.08386\"})\n",
            "\n",
            " Tool Output:\n",
            "Published: 2016-05-26\n",
            "Title: Heat-bath random walks with Markov bases\n",
            "Authors: Caprice Stanley, Tobias Windisch\n",
            "Summary: Graphs on lattice points are studied whose edges come from a finite set of\n",
            "allo\n",
            "\n",
            " Final Answer:\n",
            "The paper 1605.08386, titled \"Heat-bath random walks with Markov bases\" by Caprice Stanley and Tobias Windisch, is about graphs on lattice points whose edges come from a finite set.\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R92wEq4P7Xen"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}